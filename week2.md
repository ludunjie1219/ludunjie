# Week2
## 1. 修改上周的代码，探索GCN的层数、隐藏层神经元的数量、优化器的学习率和weight decay分别会对分类准确率产生什么影响？
* **GCN层数对准确率的影响：**
| 层数layer | 2 | 3 | 4 | 5 |
|---------|---------|---------|---------|---------|
| 准确率accuracy | 0.823  | 0.783 | 0.742 | 0.710 |
| 损失loss | 0.744  | 0.673 | 1.024 | 1.506 |
  * 可以看到GCN层数增加的时候，模型的准确率会不断下降，误差先降低后上升。


* **GCN隐藏层点数对准确率的影响**：
| 隐藏层1点数 | 16 | 16 | 16 | 16 |
|---------|---------|---------|---------|---------|
| 隐藏层2点数 | 16 | 24 | 32 | 48 |
| 准确率accuracy | 0.731  | 0.748 | 0.753 | 0.796 |
| 损失loss | 0.930  | 0.879 | 0.86 | 0.674 |
  * 可以看到，隐藏层的点数增加，准确率提高，误差降低，拟合效果变好 

* **GCN优化器学习率对准确率的影响**：
| 优化器学习率 | 0.01 | 0.02 | 0.03 | 0.04 | 0.05 | 0.10 | 0.15 | 0.20 |
|---------|---------|---------|---------|---------|---------|---------|---------|---------|
| 准确率accuracy | 0.833  | 0.836 | 0.831 | 0.832 | 0.833 | 0.845 | 0.835 | 0.818 |
| 损失loss | 0.729  | 0.649 | 0.605 | 0.595 | 0.620 | 0.574 | 0.619 | 0.641 |
  * 可以看到学习率一开始提高的时候，准确率也跟着提高，误差下降。但是当达到一定数值的时候，再提高学习率反而会导致拟合效果变差

* **正则化系数weight_decay对准确率的影响**：
| weight_decay指数级别 | -6 | -5 | -4 | -3 | -2 |
|---------|---------|---------|---------|---------|---------|
| 准确率accuracy | 0.812  | 0.813 | 0.837 | 0.440 | 0.309 |
| 损失loss | 0.767  | 0.606 | 0.599 | 1.557 | 1.836 |
  * 一开始调高系数可以使准确率略微升高，然而当到达-4指数级别时，再升高系数会使得准确率快速降低


## 2. 学习使用Optuna框架，并搜索GCN分别在Cora和Citeseer上的最佳超参数组合。使用该框架中的可视化方法分析哪些超参数对准确率的影响最大。

1. **定义优化目标 (`objective` 函数)**: 
    - 这是 Optuna 的核心，它定义了如何使用一组超参数来训练模型，并返回一个值来表示这组超参数的效果。在这个例子中，我们返回验证集上的准确率。
    - 这个函数中，首先定义了超参数的范围和可能的值。
    - 然后，使用这组超参数来设置模型和训练过程。
    - 最后，返回验证准确率作为目标值。
2. **创建 Optuna 学习任务 (`study`)**: 
    - 使用 `optuna.create_study()` 创建一个新的学习任务（或`study`），指定目标是最大化目标函数的值（在这个例子中，是验证准确率）。
3. **执行超参数搜索**: 
    - 使用 `study.optimize` 方法来执行超参数搜索。这会多次调用 `objective` 函数，每次都使用一个新的超参数组合。
    - `n_trials=100` 表示 Optuna 将尝试 100 次不同的超参数组合。
4. **打印最佳结果**: 
    - 代码会输出最佳的试验结果，包括最高的验证准确率和达到这个效果的超参数值。
5. **可视化超参数的重要性**: 
    - 使用 Optuna 的可视化工具来生成一个图，显示每个超参数对目标函数（验证准确率）的重要性。

* cora数据集
  * 通过一百次测试可以找到最佳参数组合如下 
    ```python
    Best trial:
    Value:  0.8466666666666667
    Params: 
        lr: 0.00570584101211473
        weight_decay: 3.464996150137171e-05
        hidden_units: 18
        dropout_rate: 0.5614293294593149
    ```